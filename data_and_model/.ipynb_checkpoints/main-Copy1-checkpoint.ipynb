{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters setting\n",
    "\n",
    "#Define the path to your datafolder below\n",
    "your_datapath = ''\n",
    "\n",
    "#Define search space for number of trees in random forest and depth of trees\n",
    "num_trees_min = 8\n",
    "num_trees_max = 9\n",
    "\n",
    "depth_min = 4\n",
    "depth_max = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>district_x</th>\n",
       "      <th>population</th>\n",
       "      <th>Under-Five Population</th>\n",
       "      <th>GAM</th>\n",
       "      <th>MAM</th>\n",
       "      <th>SAM</th>\n",
       "      <th>prevalence</th>\n",
       "      <th>SAM Prevalence</th>\n",
       "      <th>...</th>\n",
       "      <th>n_protests</th>\n",
       "      <th>n_riots</th>\n",
       "      <th>n_strategicdev</th>\n",
       "      <th>n_violcivilians</th>\n",
       "      <th>n_conflict_total</th>\n",
       "      <th>prevalence_6lag</th>\n",
       "      <th>next_prevalence</th>\n",
       "      <th>month</th>\n",
       "      <th>increase</th>\n",
       "      <th>increase_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>606</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>Afgooye</td>\n",
       "      <td>309683.385000</td>\n",
       "      <td>61936.677000</td>\n",
       "      <td>22867.021148</td>\n",
       "      <td>17713.889622</td>\n",
       "      <td>5153.131526</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>532</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Afgooye</td>\n",
       "      <td>309683.385000</td>\n",
       "      <td>61936.677000</td>\n",
       "      <td>21739.773627</td>\n",
       "      <td>16747.677461</td>\n",
       "      <td>4992.096166</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>0.384859</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.033859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>453</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>Afgooye</td>\n",
       "      <td>262835.818868</td>\n",
       "      <td>52567.163774</td>\n",
       "      <td>20230.949240</td>\n",
       "      <td>16460.215189</td>\n",
       "      <td>3770.734052</td>\n",
       "      <td>0.384859</td>\n",
       "      <td>0.071732</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.438340</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>0.053481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>375</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Afgooye</td>\n",
       "      <td>262835.818868</td>\n",
       "      <td>52567.163774</td>\n",
       "      <td>23042.308800</td>\n",
       "      <td>18830.718275</td>\n",
       "      <td>4211.590525</td>\n",
       "      <td>0.438340</td>\n",
       "      <td>0.080118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.384859</td>\n",
       "      <td>0.462159</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.023819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>297</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>Afgooye</td>\n",
       "      <td>262835.818868</td>\n",
       "      <td>52567.163774</td>\n",
       "      <td>24294.394753</td>\n",
       "      <td>20573.683920</td>\n",
       "      <td>3720.710833</td>\n",
       "      <td>0.462159</td>\n",
       "      <td>0.070780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.438340</td>\n",
       "      <td>0.449223</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.012937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>260</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Afgooye</td>\n",
       "      <td>191430.835956</td>\n",
       "      <td>38286.167191</td>\n",
       "      <td>17199.009189</td>\n",
       "      <td>14577.634376</td>\n",
       "      <td>2621.374813</td>\n",
       "      <td>0.449223</td>\n",
       "      <td>0.068468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.462159</td>\n",
       "      <td>0.458235</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.009013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>186</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>Afgooye</td>\n",
       "      <td>191431.000000</td>\n",
       "      <td>38286.000000</td>\n",
       "      <td>17544.000000</td>\n",
       "      <td>14278.000000</td>\n",
       "      <td>3266.000000</td>\n",
       "      <td>0.458235</td>\n",
       "      <td>0.085305</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.449223</td>\n",
       "      <td>0.484132</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>0.025897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>118</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>Afgooye</td>\n",
       "      <td>191431.000000</td>\n",
       "      <td>38285.000000</td>\n",
       "      <td>18535.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3395.000000</td>\n",
       "      <td>0.484132</td>\n",
       "      <td>0.088677</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.458235</td>\n",
       "      <td>0.463764</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.020368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>Afgooye</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94444.600000</td>\n",
       "      <td>43800.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8930.000000</td>\n",
       "      <td>0.463764</td>\n",
       "      <td>0.094553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.484132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       date district_x     population  Under-Five Population  \\\n",
       "0         606 2017-07-01    Afgooye  309683.385000           61936.677000   \n",
       "1         532 2018-01-01    Afgooye  309683.385000           61936.677000   \n",
       "2         453 2018-07-01    Afgooye  262835.818868           52567.163774   \n",
       "3         375 2019-01-01    Afgooye  262835.818868           52567.163774   \n",
       "4         297 2019-07-01    Afgooye  262835.818868           52567.163774   \n",
       "5         260 2020-01-01    Afgooye  191430.835956           38286.167191   \n",
       "6         186 2020-07-01    Afgooye  191431.000000           38286.000000   \n",
       "7         118 2021-01-01    Afgooye  191431.000000           38285.000000   \n",
       "8           1 2021-07-01    Afgooye            NaN           94444.600000   \n",
       "\n",
       "            GAM           MAM          SAM  prevalence  SAM Prevalence  ...  \\\n",
       "0  22867.021148  17713.889622  5153.131526    0.369200        0.083200  ...   \n",
       "1  21739.773627  16747.677461  4992.096166    0.351000        0.080600  ...   \n",
       "2  20230.949240  16460.215189  3770.734052    0.384859        0.071732  ...   \n",
       "3  23042.308800  18830.718275  4211.590525    0.438340        0.080118  ...   \n",
       "4  24294.394753  20573.683920  3720.710833    0.462159        0.070780  ...   \n",
       "5  17199.009189  14577.634376  2621.374813    0.449223        0.068468  ...   \n",
       "6  17544.000000  14278.000000  3266.000000    0.458235        0.085305  ...   \n",
       "7  18535.000000           NaN  3395.000000    0.484132        0.088677  ...   \n",
       "8  43800.000000           NaN  8930.000000    0.463764        0.094553  ...   \n",
       "\n",
       "   n_protests n_riots n_strategicdev  n_violcivilians  n_conflict_total  \\\n",
       "0         0.0     0.0            3.0             22.0             109.0   \n",
       "1         3.0     1.0           12.0             25.0             166.0   \n",
       "2         1.0     1.0            6.0             19.0             120.0   \n",
       "3         0.0     1.0            0.0             13.0             103.0   \n",
       "4         0.0     0.0           11.0             15.0              79.0   \n",
       "5         0.0     1.0            4.0             19.0             127.0   \n",
       "6         1.0     0.0            2.0             21.0             135.0   \n",
       "7         2.0     0.0            2.0             14.0             141.0   \n",
       "8         0.0     0.0            1.0              9.0             175.0   \n",
       "\n",
       "   prevalence_6lag  next_prevalence  month  increase  increase_numeric  \n",
       "0              NaN         0.351000      7     False         -0.018200  \n",
       "1         0.369200         0.384859      1      True          0.033859  \n",
       "2         0.351000         0.438340      7      True          0.053481  \n",
       "3         0.384859         0.462159      1      True          0.023819  \n",
       "4         0.438340         0.449223      7     False         -0.012937  \n",
       "5         0.462159         0.458235      1      True          0.009013  \n",
       "6         0.449223         0.484132      7      True          0.025897  \n",
       "7         0.458235         0.463764      1     False         -0.020368  \n",
       "8         0.484132              NaN      7       NaN               NaN  \n",
       "\n",
       "[9 rows x 81 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = ''\n",
    "district_name = 'Afgooye'\n",
    "prevalence_df = pd.read_csv(datapath + 'prevalence_estimates.csv', parse_dates=['date'])\n",
    "ipc_df = pd.read_csv(datapath + 'ipc2.csv', parse_dates=['date'])\n",
    "risk_df = pd.read_csv(datapath + 'FSNAU_riskfactors.csv', parse_dates=['date'])\n",
    "production_df = pd.read_csv(datapath + 'production.csv', parse_dates=['date'])\n",
    "    \n",
    "admissions_df = pd.read_csv(datapath + 'admissions.csv', parse_dates=['date'])\n",
    "conflict_df = pd.read_csv(datapath + 'conflict.csv', parse_dates=['date'])\n",
    "    \n",
    "#date only\n",
    "covid_df = pd.read_csv(datapath + 'covid.csv', parse_dates=['date'])\n",
    "    \n",
    "    \n",
    "#Select data for specific district\n",
    "prevalence_df = prevalence_df[prevalence_df['district']==district_name]\n",
    "ipc_df = ipc_df[ipc_df['district']==district_name]\n",
    "risk_df = risk_df[risk_df['district']==district_name]\n",
    "production_df = production_df[production_df['district']==district_name]\n",
    "admissions_df = admissions_df[admissions_df['district']==district_name]\n",
    "conflict_df = conflict_df[conflict_df['district']==district_name]\n",
    "    \n",
    "conflict_df.fillna(0,inplace=True)\n",
    "production_df.fillna(0,inplace=True)\n",
    "    \n",
    "#GroupBy \"key\", 6M = 6 months, x.replace(day=1) = the first day of that month\n",
    "risk_df = risk_df.groupby(pd.Grouper(key='date', freq='6M')).mean() \n",
    "risk_df = risk_df.reset_index()\n",
    "risk_df['date'] = risk_df['date'].apply(lambda x : x.replace(day=1))\n",
    "    \n",
    "covid_df = covid_df.groupby(pd.Grouper(key='date', freq='6M')).sum()\n",
    "covid_df = covid_df.reset_index()\n",
    "covid_df['date'] = covid_df['date'].apply(lambda x : x.replace(day=1))\n",
    "    \n",
    "conflict_df = conflict_df.groupby(pd.Grouper(key='date', freq='6M')).sum()\n",
    "conflict_df = conflict_df.reset_index()\n",
    "conflict_df['date'] = conflict_df['date'].apply(lambda x : x.replace(day=1))\n",
    "    \n",
    "admissions_df = admissions_df.groupby(pd.Grouper(key='date', freq='6M')).sum()\n",
    "admissions_df = admissions_df.reset_index()\n",
    "admissions_df['date'] = admissions_df['date'].apply(lambda x : x.replace(day=1))\n",
    "\n",
    "production_df = production_df.groupby(pd.Grouper(key='date', freq='6M')).sum()\n",
    "production_df = production_df.reset_index()\n",
    "production_df['date'] = production_df['date'].apply(lambda x : x.replace(day=1))\n",
    "\n",
    "#production_df['cropdiv'] = production_df.count(axis=1)\n",
    "    \n",
    "#Sort dataframes on date\n",
    "prevalence_df.sort_values('date', inplace=True)\n",
    "covid_df.sort_values('date', inplace=True)\n",
    "ipc_df.sort_values('date', inplace=True)\n",
    "risk_df.sort_values('date', inplace=True)\n",
    "production_df.sort_values('date', inplace=True)\n",
    "admissions_df.sort_values('date', inplace=True)\n",
    "conflict_df.sort_values('date', inplace=True)\n",
    "    \n",
    "\n",
    "#Merge dataframes, only joining on current or previous dates as to prevent data leakage\n",
    "df = pd.merge_asof(left=prevalence_df, right=ipc_df, direction='backward', on='date')\n",
    "df = pd.merge_asof(left=df, right=production_df, direction='backward', on='date')\n",
    "df = pd.merge_asof(left=df, right=risk_df, direction='backward', on='date')\n",
    "df = pd.merge_asof(left=df, right=covid_df, direction='backward', on='date')\n",
    "    \n",
    "df = pd.merge_asof(left=df, right=admissions_df, direction='backward', on='date')\n",
    "df = pd.merge_asof(left=df, right=conflict_df, direction='backward', on='date')\n",
    "    \n",
    "    \n",
    "#Calculate prevalence 6lag\n",
    "df['prevalence_6lag'] = df['GAM Prevalence'].shift(1)\n",
    "df['next_prevalence'] = df['GAM Prevalence'].shift(-1)\n",
    "\n",
    "df.rename(columns={\"GAM Prevalence\": \"prevalence\", \"new_cases\": \"covid\", \"ndvi_score\": \"ndvi\", \"phase3plus_perc\": \"ipc\", \"total population\": \"population\"}, inplace = True)\n",
    "    \n",
    "    \n",
    "#Add month column\n",
    "df['month'] = df['date'].dt.month\n",
    "    \n",
    "#Add target variable: increase for next month prevalence (boolean)\n",
    "increase = [False if x[1]<x[0] else True for x in list(zip(df['prevalence'], df['prevalence'][1:]))]\n",
    "increase.append(False)\n",
    "df['increase'] = increase\n",
    "df.iloc[-1, df.columns.get_loc('increase')] = np.nan #No info on next month\n",
    "    \n",
    "#Add target variable: increase for next month prevalence (boolean)\n",
    "increase_numeric = [x[1] - x[0] for x in list(zip(df['prevalence'], df['prevalence'][1:]))]\n",
    "increase_numeric.append(0)\n",
    "df['increase_numeric'] = increase_numeric\n",
    "df.iloc[-1, df.columns.get_loc('increase_numeric')] = np.nan #No info on next month\n",
    "    \n",
    "df.loc[(df.date < pd.to_datetime('2020-03-01')), 'covid'] = 0\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates a pandas dataframe for a single district with columns for the baseline model with semiyearly entries\n",
    "def make_district_df_semiyearly(datapath, district_name):\n",
    "    \"\"\"\n",
    "    Function that creates a pandas dataframe for a single district with columns for the baseline model with semiyearly entries\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapath : string\n",
    "        Path to the datafolder\n",
    "    district_name : string\n",
    "        Name of the district\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "\t#Read all relevant datasets\n",
    "    #date and district \n",
    "    prevalence_df = pd.read_csv(datapath + 'prevalence_estimates.csv', parse_dates=['date'])\n",
    "    ipc_df = pd.read_csv(datapath + 'ipc2.csv', parse_dates=['date'])\n",
    "    risk_df = pd.read_csv(datapath + 'FSNAU_riskfactors.csv', parse_dates=['date'])\n",
    "    production_df = pd.read_csv(datapath + 'production.csv', parse_dates=['date'])\n",
    "    \n",
    "    admissions_df = pd.read_csv(datapath + 'admissions.csv', parse_dates=['date'])\n",
    "    conflict_df = pd.read_csv(datapath + 'conflict.csv', parse_dates=['date'])\n",
    "    \n",
    "    #date only\n",
    "    covid_df = pd.read_csv(datapath + 'covid.csv', parse_dates=['date'])\n",
    "    \n",
    "    \n",
    "    #Select data for specific district\n",
    "    prevalence_df = prevalence_df[prevalence_df['district']==district_name]\n",
    "    ipc_df = ipc_df[ipc_df['district']==district_name]\n",
    "    risk_df = risk_df[risk_df['district']==district_name]\n",
    "    production_df = production_df[production_df['district']==district_name]\n",
    "    admissions_df = admissions_df[admissions_df['district']==district_name]\n",
    "    conflict_df = conflict_df[conflict_df['district']==district_name]\n",
    "    \n",
    "    conflict_df.fillna(0,inplace=True)\n",
    "    production_df.fillna(0,inplace=True)\n",
    "    \n",
    "    #GroupBy \"key\", 6M = 6 months, x.replace(day=1) = the first day of that month\n",
    "    risk_df = risk_df.groupby(pd.Grouper(key='date', freq='6M')).mean() \n",
    "    risk_df = risk_df.reset_index()\n",
    "    risk_df['date'] = risk_df['date'].apply(lambda x : x.replace(day=1))\n",
    "    \n",
    "    covid_df = covid_df.groupby(pd.Grouper(key='date', freq='6M')).sum()\n",
    "    covid_df = covid_df.reset_index()\n",
    "    covid_df['date'] = covid_df['date'].apply(lambda x : x.replace(day=1))\n",
    "    \n",
    "    conflict_df = conflict_df.groupby(pd.Grouper(key='date', freq='6M')).sum()\n",
    "    conflict_df = conflict_df.reset_index()\n",
    "    conflict_df['date'] = conflict_df['date'].apply(lambda x : x.replace(day=1))\n",
    "    \n",
    "    admissions_df = admissions_df.groupby(pd.Grouper(key='date', freq='6M')).sum()\n",
    "    admissions_df = admissions_df.reset_index()\n",
    "    admissions_df['date'] = admissions_df['date'].apply(lambda x : x.replace(day=1))\n",
    "\n",
    "    production_df = production_df.groupby(pd.Grouper(key='date', freq='6M')).sum()\n",
    "    production_df = production_df.reset_index()\n",
    "    production_df['date'] = production_df['date'].apply(lambda x : x.replace(day=1))\n",
    "    \n",
    "    #production_df['cropdiv'] = production_df.count(axis=1)\n",
    "    \n",
    "    #Sort dataframes on date\n",
    "    prevalence_df.sort_values('date', inplace=True)\n",
    "    covid_df.sort_values('date', inplace=True)\n",
    "    ipc_df.sort_values('date', inplace=True)\n",
    "    risk_df.sort_values('date', inplace=True)\n",
    "    production_df.sort_values('date', inplace=True)\n",
    "    admissions_df.sort_values('date', inplace=True)\n",
    "    conflict_df.sort_values('date', inplace=True)\n",
    "    \n",
    "\n",
    "    #Merge dataframes, only joining on current or previous dates as to prevent data leakage\n",
    "    df = pd.merge_asof(left=prevalence_df, right=ipc_df, direction='backward', on='date')\n",
    "    df = pd.merge_asof(left=df, right=production_df, direction='backward', on='date')\n",
    "    df = pd.merge_asof(left=df, right=risk_df, direction='backward', on='date')\n",
    "    df = pd.merge_asof(left=df, right=covid_df, direction='backward', on='date')\n",
    "    \n",
    "    df = pd.merge_asof(left=df, right=admissions_df, direction='backward', on='date')\n",
    "    df = pd.merge_asof(left=df, right=conflict_df, direction='backward', on='date')\n",
    "    \n",
    "    \n",
    "    #Calculate prevalence 6lag\n",
    "    df['prevalence_6lag'] = df['GAM Prevalence'].shift(1)\n",
    "    df['next_prevalence'] = df['GAM Prevalence'].shift(-1)\n",
    "    \n",
    "    '''    \n",
    "    #Select needed columns\n",
    "    df = df[['date', 'district', 'GAM Prevalence', 'next_prevalence', 'prevalence_6lag', 'new_cases', 'ndvi_score', 'phase3plus_perc', 'cropdiv', 'total population']]\n",
    "    df.columns = ['date', 'district', 'prevalence', 'next_prevalence', 'prevalence_6lag', 'covid', 'ndvi', 'ipc', 'cropdiv', 'population']\n",
    "    '''\n",
    "    df.rename(columns={\"GAM Prevalence\": \"prevalence\", \"new_cases\": \"covid\", \"ndvi_score\": \"ndvi\", \"phase3plus_perc\": \"ipc\", \"total population\": \"population\"}, inplace = True)\n",
    "    \n",
    "    #Add month column\n",
    "    df['month'] = df['date'].dt.month\n",
    "    \n",
    "    #Add target variable: increase for next month prevalence (boolean)\n",
    "    increase = [False if x[1]<x[0] else True for x in list(zip(df['prevalence'], df['prevalence'][1:]))]\n",
    "    increase.append(False)\n",
    "    df['increase'] = increase\n",
    "    df.iloc[-1, df.columns.get_loc('increase')] = np.nan #No info on next month\n",
    "    \n",
    "    #Add target variable: increase for next month prevalence (boolean)\n",
    "    increase_numeric = [x[1] - x[0] for x in list(zip(df['prevalence'], df['prevalence'][1:]))]\n",
    "    increase_numeric.append(0)\n",
    "    df['increase_numeric'] = increase_numeric\n",
    "    df.iloc[-1, df.columns.get_loc('increase_numeric')] = np.nan #No info on next month\n",
    "    \n",
    "    df.loc[(df.date < pd.to_datetime('2020-03-01')), 'covid'] = 0\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that combines the semiyearly dataset (from the function make_district_df_semiyearly) of all districts\n",
    "def make_combined_df_semiyearly(datapath):\n",
    "    \"\"\"\n",
    "    Function that creates a pandas dataframe for all districts with columns for the baseline model with semiyearly entries\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapath : string\n",
    "        Path to the datafolder\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    prevdf = pd.read_csv(datapath + 'prevalence_estimates.csv', parse_dates=['date'])\n",
    "    districts = prevdf['district'].unique()\n",
    "    \n",
    "    df_list = []\n",
    "    for district in districts:\n",
    "        district_df = make_district_df_semiyearly(datapath, district)\n",
    "        district_df['district'] = district\n",
    "        df_list.append(district_df)\n",
    "        \n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df['district_encoded'] = df['district'].astype('category').cat.codes\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that returns every possible subset (except the empty set) of the input list l\n",
    "def subsets (l):\n",
    "    subset_list = []\n",
    "    for i in range(len(l) + 1):\n",
    "        for j in range(i):\n",
    "            subset_list.append(l[j: i])\n",
    "    return subset_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d2268c03c121>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m'''------------SECTION DATAFRAME CREATION--------------'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Create the dataframe for all districts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_combined_df_semiyearly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myour_datapath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Drop every row with missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-34fa72b766af>\u001b[0m in \u001b[0;36mmake_combined_df_semiyearly\u001b[1;34m(datapath)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mdf_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdistrict\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdistricts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mdistrict_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_district_df_semiyearly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistrict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mdistrict_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'district'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistrict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mdf_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistrict_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-1be3566815aa>\u001b[0m in \u001b[0;36mmake_district_df_semiyearly\u001b[1;34m(datapath, district_name)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mcovid_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcovid_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mconflict_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconflict_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGrouper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'6M'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mconflict_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconflict_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mconflict_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconflict_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   7710\u001b[0m         \u001b[1;31m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7711\u001b[0m         \u001b[1;31m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7712\u001b[1;33m         return DataFrameGroupBy(\n\u001b[0m\u001b[0;32m   7713\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7714\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    880\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[0;32m    883\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[1;31m# a passed-in Grouper, directly convert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m         \u001b[0mbinner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_grouper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\resample.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[1;34m(self, obj, validate)\u001b[0m\n\u001b[0;32m   1605\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_grouper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1606\u001b[0m         \u001b[1;31m# create the resampler and return our binner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1607\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_resampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1608\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\resample.py\u001b[0m in \u001b[0;36m_get_resampler\u001b[1;34m(self, obj, kind)\u001b[0m\n\u001b[0;32m   1597\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mTimedeltaIndexResampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;34m\"Only valid with DatetimeIndex, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m             \u001b[1;34m\"TimedeltaIndex or PeriodIndex, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'"
     ]
    }
   ],
   "source": [
    "'''------------SECTION DATAFRAME CREATION--------------'''\n",
    "#Create the dataframe for all districts\n",
    "df = make_combined_df_semiyearly(your_datapath)\n",
    "\n",
    "#Drop every row with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#Sort dataframe on date and reset the index\n",
    "df.sort_values('date', inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#Drop disctricts with less than 7 observations: 'Burco', 'Saakow', 'Rab Dhuure', 'Baydhaba', 'Afmadow'\n",
    "df.drop(df[df['district'].isin(['Burco', 'Saakow', 'Rab Dhuure', 'Baydhaba', 'Afmadow'])].index, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''------------SECTION RANDOM FOREST CROSS VALIDATION--------------'''\n",
    "#WARNING: this process can take some time, since there are a lot of hyperparameters to investigate. The search space can be manually reduced to speed up the process.\n",
    "\n",
    "#Create empty list to store model scores\n",
    "parameter_scores = []\n",
    "\n",
    "#Define target and explanatory variables\n",
    "X = df.drop(columns = ['increase', 'increase_numeric', 'date', 'district', 'prevalence', 'next_prevalence']) #Note that these columns are dropped, the remaining columns are used as explanatory variables\n",
    "y = df['next_prevalence'].values\n",
    "\n",
    "for num_trees in range(num_trees_min, num_trees_max):\n",
    "    \n",
    "    for depth in range(depth_min, depth_max):\n",
    "        \n",
    "        #Investigate every subset of explanatory variables\n",
    "        for features in subsets(X.columns):\n",
    "        \n",
    "            #First CV split. The 99 refers to the first 3 observations for the 33 districts in the data.\n",
    "            Xtrain = X[:99][features].copy().values\n",
    "            ytrain = y[:99]\n",
    "            Xtest = X[99:132][features].copy().values\n",
    "            ytest = y[99:132]\n",
    "\n",
    "            #Create a RandomForestRegressor with the selected hyperparameters and random state 0.\n",
    "            clf = RandomForestRegressor(n_estimators=num_trees, max_depth=depth, random_state=0)\n",
    "\n",
    "            #Fit to the training data\n",
    "            clf.fit(Xtrain, ytrain)\n",
    "\n",
    "            #Make a prediction on the test data\n",
    "            predictions = clf.predict(Xtest)\n",
    "\n",
    "            #Calculate mean absolute error\n",
    "            MAE1 = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "\n",
    "            #Second CV split. The 132 refers to the first 4 observations for the 33 districts in the data.\n",
    "            Xtrain = X[:132][features].copy().values\n",
    "            ytrain = y[:132]\n",
    "            Xtest = X[132:165][features].copy().values\n",
    "            ytest = y[132:165]\n",
    "\n",
    "            #Create a RandomForestRegressor with the selected hyperparameters and random state 0.\n",
    "            clf = RandomForestRegressor(n_estimators=num_trees, max_depth=depth, random_state=0)\n",
    "\n",
    "            #Fit to the training data\n",
    "            clf.fit(Xtrain, ytrain)\n",
    "\n",
    "            #Make a prediction on the test data\n",
    "            predictions = clf.predict(Xtest)\n",
    "\n",
    "            #Calculate mean absolute error\n",
    "            MAE2 = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "            #Calculate the mean MAE over the two folds\n",
    "            mean_MAE = (MAE1 + MAE2)/2\n",
    "\n",
    "            #Store the mean MAE together with the used hyperparameters in list \n",
    "            parameter_scores.append((mean_MAE, num_trees, depth, features))\n",
    "\n",
    "#Sort the models based on score and retrieve the hyperparameters of the best model\n",
    "parameter_scores.sort(key=lambda x: x[0])\n",
    "best_model_score = parameter_scores[0][0]\n",
    "best_model_trees = parameter_scores[0][1]\n",
    "best_model_depth = parameter_scores[0][2]\n",
    "best_model_columns = list(parameter_scores[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of trees: 5\n",
      "max_depth: 2\n",
      "columns: ['ipc', 'cropdiv', 'population', 'month', 'district_encoded']\n",
      "0.06461966113961246 0.8250688705234159\n"
     ]
    }
   ],
   "source": [
    "'''------------SECTION FINAL EVALUATION--------------'''\n",
    "X = df[best_model_columns].values\n",
    "y = df['next_prevalence'].values\n",
    "\n",
    "#If there is only one explanatory variable, the values need to be reshaped for the model\n",
    "if len(best_model_columns) == 1:\n",
    "\tX = X.reshape(-1, 1)\n",
    "\n",
    "#Peform evaluation on full data\n",
    "Xtrain = X[:165]\n",
    "ytrain = y[:165]\n",
    "Xtest = X[165:]\n",
    "ytest = y[165:]\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=best_model_trees, max_depth=best_model_depth, random_state=0)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "predictions = clf.predict(Xtest)\n",
    "\n",
    "#Calculate MAE\n",
    "MAE = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "#Generate boolean values for increase or decrease in prevalence. 0 if next prevalence is smaller than current prevalence, 1 otherwise.\n",
    "increase           = [0 if x<y else 1 for x in df.iloc[165:]['next_prevalence'] for y in df.iloc[165:]['prevalence']]\n",
    "predicted_increase = [0 if x<y else 1 for x in predictions                      for y in df.iloc[165:]['prevalence']]\n",
    "\n",
    "#Calculate accuracy of predicted boolean increase/decrease\n",
    "acc = accuracy_score(increase, predicted_increase)\n",
    "\n",
    "#Print model parameters\n",
    "print('no. of trees: ' + str(best_model_trees) + '\\nmax_depth: ' + str(best_model_depth) + '\\ncolumns: ' + str(best_model_columns))\n",
    "\n",
    "#Print model scores\n",
    "print(MAE, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
